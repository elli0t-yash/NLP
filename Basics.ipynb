{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7920716d",
   "metadata": {},
   "source": [
    "# NLP (Natural Language Processing)\n",
    "NLP is a subfield of computer science and artificial intelligence concerned with interactions between computers and human (natural) languages. It is used to apply machine learning algorithms to text and speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542bd36",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e604985",
   "metadata": {},
   "source": [
    "# The Basics of NLP for Text\n",
    "\n",
    "1. Sentence Tokenization\n",
    "2. Word Tokenization\n",
    "3. Text Lemmatization and Stemming\n",
    "4. Stop Words\n",
    "5. Regex\n",
    "6. Bag-of-Words\n",
    "7. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abcfa017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df9382",
   "metadata": {},
   "source": [
    "## Sentence Tokenization:\n",
    "* Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. \n",
    "* we can split apart the sentences whenever we see a punctuation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f76e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon is one of the oldest known board games.\n",
      "\n",
      "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "\n",
      "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c03215",
   "metadata": {},
   "source": [
    "## Word Tokenization:\n",
    "* Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95465ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
      "\n",
      "['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
      "\n",
      "['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f60479",
   "metadata": {},
   "source": [
    "## Text Lemmatization and Stemming:\n",
    "\n",
    "* The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "\n",
    "* Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.\n",
    "        \n",
    "\n",
    "* Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "        \n",
    "\n",
    "* stemmer operates without knowledge of the context and easier to implement and usually run faster. \n",
    "        \n",
    "\n",
    "* The word “better” has “good” as its lemma. This link is missed by stemming, as it requires a dictionary look-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "409c2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer care\n",
      "Lemmatizer care\n",
      "\n",
      "Stemmer drove\n",
      "Lemmatizer drive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def comp_stem_and_lem(stemmer, lemmatizer, word, pos):\n",
    "    print(\"Stemmer\", stemmer.stem(word))\n",
    "    print(\"Lemmatizer\", lemmatizer.lemmatize(word, pos))\n",
    "    print()\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "comp_stem_and_lem(stemmer, lemmatizer, word=\"caring\", pos=wordnet.VERB)\n",
    "comp_stem_and_lem(stemmer, lemmatizer, word=\"drove\", pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3869951c",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "* Stop words are words which are filtered out before or after processing of text. When applying machine learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0da4ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a910a",
   "metadata": {},
   "source": [
    "## Optimized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfd790b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n",
      "['Its', 'history', 'traced', 'back', 'nearly', '5,000', 'years', 'archeological', 'discoveries', 'Middle', 'East', '.']\n",
      "['It', 'two', 'player', 'game', 'player', 'fifteen', 'checkers', 'move', 'twenty-four', 'points', 'according', 'roll', 'two', 'dice', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    without_stop_words = [word for word in words if not word in stop_words]\n",
    "    print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991fd41a",
   "metadata": {},
   "source": [
    "## Regex\n",
    "* A regular expression, regex, or regexp is a sequence of characters that define a search pattern.\n",
    "\n",
    "* We can use regex to apply additional filtering to our text. For example, we can remove all the non-words characters. In many cases, we don’t need the punctuation marks and it’s easy to remove them with regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e61ee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon is one of the oldest known board games \n",
      "Its history can be traced back nearly 5 000 years to archeological discoveries in the Middle East \n",
      "It is a two player game where each player has fifteen checkers which move between twenty four points according to the roll of two dice \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for sentence in sentences:\n",
    "    pattern = r\"[^\\w]\"\n",
    "    print(re.sub(pattern, \" \", sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25eb056",
   "metadata": {},
   "source": [
    "## Bag-of-Words\n",
    "* Bag-of-Words (BoW) model is a simple yet powerful technique for representing text data numerically\n",
    "\n",
    "* Steps:\n",
    "    * Text Preprocessing\n",
    "    * Vocabulary Creation\n",
    "    * Feature Extraction\n",
    "    * Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f434a605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['000' 'according' 'archeological' 'back' 'backgammon' 'be' 'between'\n",
      " 'board' 'can' 'checkers' 'dice' 'discoveries' 'each' 'east' 'fifteen'\n",
      " 'four' 'game' 'games' 'has' 'history' 'in' 'is' 'it' 'its' 'known'\n",
      " 'middle' 'move' 'nearly' 'of' 'oldest' 'one' 'player' 'points' 'roll'\n",
      " 'the' 'to' 'traced' 'twenty' 'two' 'where' 'which' 'years']\n",
      "BoW Matrix:\n",
      "[[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0\n",
      "  0 0 0 0 0 0]\n",
      " [1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1\n",
      "  1 0 0 0 0 1]\n",
      " [0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 2 1 1 1 1\n",
      "  0 1 2 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "bow_matrix_dense = bow_matrix.toarray()\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"BoW Matrix:\")\n",
    "print(bow_matrix_dense)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b37d76c",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "* TF-IDF, short for term frequency-inverse document frequency is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus.\n",
    "\n",
    "* One problem with scoring word frequency is that the most frequent words in the document start to have the highest scores. These frequent words may not contain as much “informational gain” to the model compared with some rarer and domain-specific words. One approach to fix that problem is to penalize words that are frequent across all the documents. This approach is called TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110bbe69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>according</th>\n",
       "      <th>archeological</th>\n",
       "      <th>back</th>\n",
       "      <th>backgammon</th>\n",
       "      <th>be</th>\n",
       "      <th>between</th>\n",
       "      <th>board</th>\n",
       "      <th>can</th>\n",
       "      <th>checkers</th>\n",
       "      <th>...</th>\n",
       "      <th>points</th>\n",
       "      <th>roll</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>traced</th>\n",
       "      <th>twenty</th>\n",
       "      <th>two</th>\n",
       "      <th>where</th>\n",
       "      <th>which</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.365011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.215582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.258828</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.258828</td>\n",
       "      <td>0.258828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258828</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258828</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.152868</td>\n",
       "      <td>0.196845</td>\n",
       "      <td>0.258828</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.258828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>0.115643</td>\n",
       "      <td>0.148911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>0.3916</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000  according  archeological      back  backgammon        be  \\\n",
       "0  0.000000     0.0000       0.000000  0.000000    0.365011  0.000000   \n",
       "1  0.258828     0.0000       0.258828  0.258828    0.000000  0.258828   \n",
       "2  0.000000     0.1958       0.000000  0.000000    0.000000  0.000000   \n",
       "\n",
       "   between     board       can  checkers  ...  points    roll       the  \\\n",
       "0   0.0000  0.365011  0.000000    0.0000  ...  0.0000  0.0000  0.215582   \n",
       "1   0.0000  0.000000  0.258828    0.0000  ...  0.0000  0.0000  0.152868   \n",
       "2   0.1958  0.000000  0.000000    0.1958  ...  0.1958  0.1958  0.115643   \n",
       "\n",
       "         to    traced  twenty     two   where   which     years  \n",
       "0  0.000000  0.000000  0.0000  0.0000  0.0000  0.0000  0.000000  \n",
       "1  0.196845  0.258828  0.0000  0.0000  0.0000  0.0000  0.258828  \n",
       "2  0.148911  0.000000  0.1958  0.3916  0.1958  0.1958  0.000000  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a71f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004eebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
